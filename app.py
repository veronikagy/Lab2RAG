import streamlit as st
import asyncio
import logging
from typing import Optional
import sys
from pathlib import Path

# Add src to Python path
sys.path.append(str(Path(__file__).parent / "src"))

from config import config
from src.rag_pipeline import RAGPipeline

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Page config
st.set_page_config(
    page_title="RAG –°–∏—Å—Ç–µ–º–∞",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
.main-header {
    font-size: 2.5rem;
    font-weight: bold;
    color: #1f77b4;
    text-align: center;
    margin-bottom: 2rem;
}

.section-header {
    font-size: 1.5rem;
    font-weight: bold;
    color: #333;
    margin-top: 2rem;
    margin-bottom: 1rem;
}

.stats-box {
    background-color: #f0f2f6;
    padding: 1rem;
    border-radius: 0.5rem;
    margin: 1rem 0;
}

.error-box {
    background-color: #ffebee;
    color: #c62828;
    padding: 1rem;
    border-radius: 0.5rem;
    border-left: 4px solid #c62828;
}

.success-box {
    background-color: #e8f5e8;
    color: #2e7d32;
    padding: 1rem;
    border-radius: 0.5rem;
    border-left: 4px solid #2e7d32;
}

.context-chunk {
    background-color: #f8f9fa;
    padding: 1rem;
    border-radius: 0.5rem;
    margin: 0.5rem 0;
    border-left: 3px solid #1f77b4;
}
</style>
""", unsafe_allow_html=True)

# Initialize session state
if 'rag_pipeline' not in st.session_state:
    st.session_state.rag_pipeline = None
if 'available_models' not in st.session_state:
    st.session_state.available_models = []
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []

async def initialize_rag_pipeline():
    """Initialize RAG pipeline"""
    try:
        if not config.validate():
            st.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≤ —Ñ–∞–π–ª–µ .env")
            st.stop()
        
        with st.spinner("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã..."):
            pipeline = RAGPipeline(
                openrouter_api_key=config.OPENROUTER_API_KEY,
                qdrant_url=config.QDRANT_URL,
                qdrant_api_key=config.QDRANT_API_KEY,
                collection_name=config.QDRANT_COLLECTION_NAME,
                embedding_model=config.EMBEDDING_MODEL,
                chunk_size=config.CHUNK_SIZE,
                chunk_overlap=config.CHUNK_OVERLAP
            )
            
            # Get available models
            models = await pipeline.get_available_models()
            
            return pipeline, models
            
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {str(e)}")
        return None, []

def main():
    """Main application"""
    
    # Header
    st.markdown('<div class="main-header">ü§ñ RAG –°–∏—Å—Ç–µ–º–∞</div>', unsafe_allow_html=True)
    st.markdown("---")
    
    # Initialize pipeline if needed
    if st.session_state.rag_pipeline is None:
        pipeline, models = asyncio.run(initialize_rag_pipeline())
        if pipeline:
            st.session_state.rag_pipeline = pipeline
            st.session_state.available_models = models
            st.success("RAG —Å–∏—Å—Ç–µ–º–∞ —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞!")
        else:
            st.stop()
    
    # Sidebar for configuration and file upload
    with st.sidebar:
        st.markdown('<div class="section-header">‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏</div>', unsafe_allow_html=True)
        
        # Model selection
        if st.session_state.available_models:
            model_options = [(m["id"], f"{m['name']} ({m['id']})") for m in st.session_state.available_models]
            selected_model_id = st.selectbox(
                "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å:",
                options=[opt[0] for opt in model_options],
                format_func=lambda x: next((opt[1] for opt in model_options if opt[0] == x), x),
                index=0
            )
        else:
            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π")
            selected_model_id = "openai/gpt-3.5-turbo"
        
        # RAG settings
        st.markdown("**–ù–∞—Å—Ç—Ä–æ–π–∫–∏ RAG:**")
        use_rag = st.checkbox("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å RAG", value=True)
        max_chunks = st.slider("–ú–∞–∫—Å–∏–º—É–º —á–∞–Ω–∫–æ–≤", 1, 10, 5)
        similarity_threshold = st.slider("–ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏", 0.0, 1.0, 0.1, 0.05)
        
        # Source filter
        sources = st.session_state.rag_pipeline.get_document_sources()
        source_filter = None
        if sources:
            use_source_filter = st.checkbox("–§–∏–ª—å—Ç—Ä –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É")
            if use_source_filter:
                source_filter = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç:", sources)
        
        st.markdown("---")
        
        # File upload section
        st.markdown('<div class="section-header">üìÑ –ó–∞–≥—Ä—É–∑–∫–∞ PDF</div>', unsafe_allow_html=True)
        
        uploaded_file = st.file_uploader(
            "–í—ã–±–µ—Ä–∏—Ç–µ PDF —Ñ–∞–π–ª:",
            type=['pdf'],
            help="–ó–∞–≥—Ä—É–∑–∏—Ç–µ PDF —Ñ–∞–π–ª –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"
        )
        
        chunking_strategy = st.selectbox(
            "–°—Ç—Ä–∞—Ç–µ–≥–∏—è —Ä–∞–∑–±–∏–µ–Ω–∏—è:",
            ["fixed_size", "semantic"],
            format_func=lambda x: "–§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä" if x == "fixed_size" else "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–µ–∫—Ü–∏–∏"
        )
        
        if uploaded_file is not None:
            if st.button("üöÄ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å PDF", type="primary"):
                with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ PDF —Ñ–∞–π–ª–∞..."):
                    file_content = uploaded_file.read()
                    result = asyncio.run(
                        st.session_state.rag_pipeline.process_pdf(
                            file_content, 
                            uploaded_file.name,
                            chunking_strategy
                        )
                    )
                    
                    if result["success"]:
                        st.success(f"‚úÖ –§–∞–π–ª {uploaded_file.name} —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω!")
                        st.json(result)
                    else:
                        st.error(f"‚ùå –û—à–∏–±–∫–∞: {result['error']}")
        
        st.markdown("---")
        
        # Document management
        st.markdown('<div class="section-header">üìö –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏</div>', unsafe_allow_html=True)
        
        if sources:
            st.write(f"**–î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ:** {len(sources)}")
            for source in sources:
                col1, col2 = st.columns([3, 1])
                with col1:
                    st.text(source)
                with col2:
                    if st.button("üóëÔ∏è", key=f"delete_{source}", help="–£–¥–∞–ª–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç"):
                        if st.session_state.rag_pipeline.delete_document(source):
                            st.success(f"–î–æ–∫—É–º–µ–Ω—Ç {source} —É–¥–∞–ª–µ–Ω")
                            st.rerun()
                        else:
                            st.error("–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è")
        else:
            st.info("–ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        # System stats
        if st.button("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã"):
            stats = st.session_state.rag_pipeline.get_stats()
            st.json(stats)
    
    # Main chat interface
    st.markdown('<div class="section-header">üí¨ –ß–∞—Ç —Å RAG —Å–∏—Å—Ç–µ–º–æ–π</div>', unsafe_allow_html=True)
    
    # Chat history
    for message in st.session_state.chat_history:
        if message["role"] == "user":
            with st.chat_message("user"):
                st.write(message["content"])
        else:
            with st.chat_message("assistant"):
                st.write(message["content"])
                
                # Show context if available
                if "context_chunks" in message and message["context_chunks"]:
                    with st.expander("üìñ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç"):
                        for i, chunk in enumerate(message["context_chunks"]):
                            st.markdown(f"""
                            <div class="context-chunk">
                                <strong>–ò—Å—Ç–æ—á–Ω–∏–∫:</strong> {chunk.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}<br>
                                <strong>–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å:</strong> {chunk.get('score', 0):.3f}<br>
                                <strong>–¢–µ–∫—Å—Ç:</strong> {chunk.get('text', '')[:300]}...
                            </div>
                            """, unsafe_allow_html=True)
    
    # Chat input
    if prompt := st.chat_input("–ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å..."):
        # Add user message to history
        st.session_state.chat_history.append({"role": "user", "content": prompt})
        
        # Display user message
        with st.chat_message("user"):
            st.write(prompt)
        
        # Generate response
        with st.chat_message("assistant"):
            with st.spinner("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞..."):
                result = asyncio.run(
                    st.session_state.rag_pipeline.query(
                        question=prompt,
                        model=selected_model_id,
                        max_chunks=max_chunks,
                        similarity_threshold=similarity_threshold,
                        source_filter=source_filter,
                        use_rag=use_rag
                    )
                )
                
                if result["success"]:
                    st.write(result["answer"])
                    
                    # Add to history with context
                    st.session_state.chat_history.append({
                        "role": "assistant", 
                        "content": result["answer"],
                        "context_chunks": result.get("context_chunks", []),
                        "rag_used": result.get("rag_used", False)
                    })
                    
                    # Show context
                    if result.get("context_chunks"):
                        with st.expander("üìñ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç"):
                            for i, chunk in enumerate(result["context_chunks"]):
                                st.markdown(f"""
                                <div class="context-chunk">
                                    <strong>–ò—Å—Ç–æ—á–Ω–∏–∫:</strong> {chunk.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}<br>
                                    <strong>–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å:</strong> {chunk.get('score', 0):.3f}<br>
                                    <strong>–¢–µ–∫—Å—Ç:</strong> {chunk.get('text', '')[:300]}...
                                </div>
                                """, unsafe_allow_html=True)
                    
                    # Show RAG status
                    if result.get("rag_used"):
                        st.info("‚úÖ –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RAG")
                    else:
                        st.warning("‚ö†Ô∏è –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
                        
                else:
                    error_msg = f"–û—à–∏–±–∫–∞: {result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}"
                    st.error(error_msg)
                    st.session_state.chat_history.append({
                        "role": "assistant", 
                        "content": error_msg
                    })
    
    # Clear chat button
    if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å —á–∞—Ç"):
        st.session_state.chat_history = []
        st.rerun()

if __name__ == "__main__":
    main()
